# 前提知识
   - url
   - http协议
   - web前端，html，css，js
   - ajax
   - re，xpath
   - xml

# 爬虫简介
    - 网络爬虫
    - 两大特征
        - 能按作者要求去下载数据或者内容
        - 能自动在网络山流串
    - 三大步骤
      - 下载网页
      - 提取正确的信息
      - 根据一定的规则自动跳到另外网页上执行以上两步内容
    - 爬虫分类
      - 通用爬虫
      - 专用爬虫（聚焦爬虫）
    - Python网络包简介
      - Python2.x：
      - python3.x：urllib, urllib3, httplib2,requests

- urllib
  - 包含模块
    - urllib.request:打开和读取urls
    - urllib.error：包含urllib.request产生的常见错误，使用try捕捉
    - urllib.parse：包含解析url方法
    - urllib.robotparse:解析robots.txt文件，反爬虫文件
    - 案例01
- 网页编码问题解决
  - chardet 可以自动检测页面文件的编码格式，但是可能有误
  - 案例02
- urlopen的返回对象
  - geturl:返回请求对象的url
  - info:请求反馈对象的meta信息
  - getcode:返回的http code

- request.date的使用
  - 访问网络的两种方式
    - get:
        - 利用参数给服务器传递信息
        - 参数为dict，然后使用parse模块编码
    - post
      - 一般想服务器传递参数使用
      - post是把信息自动加密处理
      - 我们如果想使用post信息，需要用到data参数
      - 使用post，意味着http的请求可能需要更改
        - content-type：application/x-www.form-urlencode
        - content-length：数据长度
        - 简而言之，一旦更改请求方式，请注意其他请求头部信息相适应
    - urllib.parse.rulencode 可以将字符串自动转化成上面的，案例03
    - 案例05
    - 为了更多的设置请求信息，单纯的urlopen函数已经不太好用了
    - 需要利用reuest.Request()
    - 案例pachong06

- urllib.error
  - URLError产生的原因
    - 没网
    - 服务器链接错误
    - 指不到指定服务器
    - 案例urlerror.py

  - HTTPError
    - 案例httperror.py

  - 两者区别
    - HTTPError是对应的HTTP请求的返回错误码，如果返回的错误码是400以上的，则引发HTTPError
    - URLError对应的一般是网络出现问题，包括url问题
    - 关系区别：OSError-URLError-HTTPError

- UserAgent
  - UserAgent：用户代理，简称UA，属于header的一部分，服务器通过UA来判断访问者身份
  - 常见的UA值，使用的时候可以直接复制粘贴，也可以访问服务器抓包看是什么值
  - 设置UA可以通过两种方式
    - headers
    - add header
    - 案例UserAgent.py

- ProxyHandler处理（代理服务器）
  - 使用代理IP，是爬虫常用手段
  - 获取代理服务器的地址
    - www.xicidaili.com
    - www.goubanjia.com
  - 代理用来隐藏真是访问中，代理也不允许频繁访问某一个固定网站，所以，代理一定要很多很多
  - 基本使用代理步骤：
    - 设置代理地址
    - 创建ProxyHandler
    - 创建Opener
    - 安装Opener
  - 案例Proxy.py

- Cookie & Session
  - 由于http协议的无记忆性，人们为了弥补这个缺陷，所采用发一个人补充协议
  - cookie是发给用户（即htttp浏览器）的一段数据，session是保存在服务器上的对应的另一半信息，用来记录用户信息
- cookie和session的区别
  - 存放位置不同
  - cookie不安全
  - session会保存在服务器山一定时间，会过期
  - 单个cookie保存数据不超过4K，很多浏览器限制一个站点最多保存20个cookie
- session的存放位置
  - 存在服务器端
  - 一般情况，session是放在内存中或者数据库中
  - 没有cookie登陆，案例cookie.py,可以看到，反馈网页为未登录状态
- 使用cookie登陆
  - 直接把cookie复制下来，然后手动放入请求头，案例cookie01.py
  - http模块包含一些关于cookie的模块，通过他们我们可以自动使用cookie
    - CookieJar
      - 管理存储cookie，向传出的http添加cookie
      - cookie存储在内存，CookieJar实例回收后cookie将消失
    - FileCookieJar（filename,delayload=None,policy=None）
      - 使用文件管理cookie
      - filename是保存cookie的文件
    - MozillaCookieJar（filename,delayload=None,policy=None）
      - 创建与mozilla浏览器cookie.txt兼容的FileCookieJar实例
    - LwpCookieJar（filename,delayload=None,policy=None）
      - 创建与libwww-perl标准兼容的set-cookie3格式的FileCookieJar实例
    - 他们的关系：CookieJar-->FileCookieJar-->MozillaCookieJar&LwpCookieJar
  - 利用cookiejar访问人人网
    - 自动使用cookie登陆，流程
      - 打开登陆页面后自动通过用户密码登陆
      - 自动提取反馈回来的cookie
      - 利用提取回来的cookie登陆隐私界面
      - 案例cookiejar.py
  - handler是Handler的实例，常用参看案例代码
    	
    - 用来处理复杂请求
    	cookie_handler = request.HTTPCookieProcessor(cookie)

		http_handler = request.HTTPHandler()

		https_handler = request.HTTPSHandler()
	- 创立handler后，使用opener打开，打开后相应的业务有相应的handler处理
	- cookie作为一个变量，打印出来，案例cookie02.py
	  - cookie属性
	    - name：名称
	    - value：值
	    - domain：可以访问此cookie的域名
	    - path：可以访问此cookie的页面路径
	    - expires：过期时间
	    - size：大小
	    - Http字段

	- cookie的保存--FileCookieJar  案例FileCookieJar.py
	- cookie的读取， 案例CookieRead.py
	  - cookie.load(filename,ignore_discard=True,ignore_expires=True)

- SSL
  - SSL证书就是指遵守SSL安全套阶层协议的服务器数字证书
  - 美国网景公司开发
  - CA是数字证书认证中心，是发放，管理，废除数字证书的收信人的第三方机构
  - 遇到不信任的SSL证书，需要单独处理，案例SSL.py

- js加密
  - 有点反爬虫策略采用js对需要传输的数据进行加密处理（通常是取md5值）
  - 经过加密，传输的就是密文，但是加密函数或者过程一定是在浏览器完成的，也就是一定会把代码（js代码）暴漏给使用者
  - 通过阅读加密算法，就可以模拟出加密过程，从而达到破解
  - 过程参考案例js.py

- ajax
  - 异步请求
  - 一定会有url，请求方法，可能有数据
  - 一般使用json格式
  - 案例ajax.py


- Requests-献给人类
  - HTTP for Humans，更简洁更友好
  - 继承了urllib的所有特性
  - 底层使用的是urllib3
  - 开源地址：
  - 中文文档：
  - 安装：conda install requests
  - get请求
    - requests.get(url)
    - requests.request("get",url)
    - 可以带有headers和parmas参数
    - 案例requests01.py

  - get返回内容
    - 案例requests02.py

  - post请求
    - rsp=requests.post(url,data=data)
    - 案例requests03.py
    - data,headers要求dict类型，不需要转码为bytes

  - proxy
    - proxies = {
    	"http":"address of proxy",
    	"https":"address of proxy"
 	   }

 	   rsp = requests.request("get","http:xxxxxx",proxies=proxies)

 	- 代理有可能报错，如果使用人数多，考虑安全问题，可能会被强行关闭

 - 用户验证
   - 代理验证 
       可能需要使用HTTP basic Auth，可以这样
       格式为 用户名：密码@代理地址
       proxy = {“http”：“china：123456@192.168.1.123：4444”}
       rsp = requests.get("http://baidu.com",proxies=proxy)
   - web客户端验证
     - 如果遇到web客户端验证，需要添加auth={用户名，密码}
         auth = （“test1”，“123456”）#授权信息
         rsp = requests.get("http://baidu.com",auth=auth)
  - cookie
    - requests可以自动处理cookie信息
        rsp = requests.get("http://xxxxx.com")
        #如果对方服务器给传递过来cookie信息，则可以通过反馈的cookie属性得到
        #返回一个cookiejar实例
        cookiejar = rsp.cookies

        #可以将cookiejar转换为字典
        cookiedict = requests.utils.dict_from_cookiejar(cookiejar)

  - session
    - 跟服务器端session不是一个东东
    - 模拟一次会话，从客户端浏览器链接服务器开始，到客户端浏览器断开
    - 能让我们跨请求时保持某些参数，比如在同一个session实例发出的所有请求之间保持cookie
         
         #创建session对象，可以保持cookie值
         ss = requests.session()

         headers = {"User-Agent":"xxxxxxxx"}

         data = {"name","xxxxxx"}

         #此时，由创建的session管理请求，负责发出请求
         ss.post("http://www.baidu.com",data=data,headers=headers)


         rsp = ss.get("xxxxxxxxx")

 - https请求验证ssl证书
   - 参数verify负责表示是否需要验证ssl证书，默认是True
   - 如果不需要验证ssl证书，则设置成False表示关闭
      rsp = requests.get("https://www.baidu.com".verify=False)
      #如果用verify=True访问12306，会报错，因为它证书有问题





